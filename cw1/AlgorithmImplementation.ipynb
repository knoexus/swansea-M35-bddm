{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023638 | Anton Dementyev | Coursework 1: Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import math\n",
    "import threading\n",
    "import pprint\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, skipEmptyColumn):\n",
    "    ds = []\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        filereader = csv.reader(csvfile, delimiter=',')\n",
    "        if skipEmptyColumn:\n",
    "            for row in filereader:\n",
    "                ## tuple is hashable, which is useful for all further actions\n",
    "                ds.append(tuple(sorted(row[:-1])))\n",
    "        else:\n",
    "            for row in filereader:\n",
    "                ds.append(tuple(sorted(row)))\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">Testing Read File</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Butter', 'Cheese', 'Coffee Powder', 'Ghee', 'Lassi', 'Yougurt'), ('Coffee Powder', 'Ghee'), ('Butter', 'Cheese', 'Lassi', 'Tea Powder'), ('Bread', 'Butter', 'Cheese', 'Coffee Powder', 'Panner', 'Tea Powder'), ('Butter', 'Cheese', 'Coffee Powder', 'Sugar', 'Sweet', 'Yougurt')]\n"
     ]
    }
   ],
   "source": [
    "read = read_file('GroceryStore.csv', True)[:5]\n",
    "print(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(data, filename, reverse):\n",
    "    # reverse mode can be helpful to create a required order of itemsets\n",
    "    if reverse:\n",
    "        data.reverse()\n",
    "    if data != [] and data is not None:\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            filewriter = csv.writer(csvfile, delimiter=',')\n",
    "            for row in data:\n",
    "                filewriter.writerow(row)\n",
    "    else:\n",
    "        print('Attempting to write empty dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support_count(instance, data, is_set):\n",
    "    count = 0\n",
    "    # if item is a plain string, do not waste memory to make convert it to a set\n",
    "    if is_set:\n",
    "        for row in data:\n",
    "            if set(instance).issubset(set(row)): \n",
    "                count = count + 1\n",
    "    else:\n",
    "        for row in data:\n",
    "            if instance in row:\n",
    "                count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support(data, items):\n",
    "    dct = {}\n",
    "    lgtn = len(data)\n",
    "    ## if not itemset is supplied, decompose the data to the mininum\n",
    "    if items == None:\n",
    "        for i in data:\n",
    "            for j in i:\n",
    "                if j in dct:\n",
    "                    continue\n",
    "                else:\n",
    "                    support_count = calculate_support_count(j, data, False)\n",
    "                    dct[j] = support_count / lgtn\n",
    "    else:\n",
    "        for i in items:\n",
    "            support_count = calculate_support_count(i, data, True)\n",
    "            dct[i] = support_count / lgtn\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_elimination(data, items, minimal_support, support_dict):\n",
    "    dct = calculate_support(data, items) if support_dict is None else support_dict\n",
    "    support_resistant = []\n",
    "    for key in dct:\n",
    "        if dct[key] >= minimal_support:\n",
    "            support_resistant.append(key)\n",
    "    return support_resistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_fk1_1(previous_step_f_itemset, step_one_f_itemset):\n",
    "    lst = []\n",
    "    if isinstance(previous_step_f_itemset[0], list) or isinstance(previous_step_f_itemset[0], tuple): \n",
    "        for i in previous_step_f_itemset:\n",
    "            for k in step_one_f_itemset:\n",
    "                if k not in i:\n",
    "                    lst.append(tuple(sorted([*i, k])))\n",
    "    else:\n",
    "        for i in previous_step_f_itemset:\n",
    "            for k in step_one_f_itemset:\n",
    "                if k != i:\n",
    "                    lst.append(tuple(sorted((i, k))))\n",
    "    return set(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Lassi', 'Panner'), ('Bread', 'Cheese'), ('Bread', 'Yougurt'), ('Coffee Powder', 'Yougurt'), ('Ghee', 'Yougurt'), ('Panner', 'Sugar'), ('Bread', 'Butter'), ('Bread', 'Sweet'), ('Bread', 'Tea Powder'), ('Bread', 'Panner'), ('Cheese', 'Sugar'), ('Coffee Powder', 'Sweet'), ('Butter', 'Sugar'), ('Ghee', 'Sweet'), ('Ghee', 'Tea Powder'), ('Milk', 'Yougurt'), ('Coffee Powder', 'Panner'), ('Ghee', 'Panner'), ('Tea Powder', 'Yougurt'), ('Coffee Powder', 'Tea Powder'), ('Cheese', 'Milk'), ('Butter', 'Milk'), ('Milk', 'Sweet'), ('Lassi', 'Sugar'), ('Milk', 'Panner'), ('Sugar', 'Yougurt'), ('Sweet', 'Yougurt'), ('Lassi', 'Milk'), ('Milk', 'Tea Powder'), ('Cheese', 'Ghee'), ('Butter', 'Ghee'), ('Cheese', 'Coffee Powder'), ('Sugar', 'Sweet'), ('Cheese', 'Lassi'), ('Sweet', 'Tea Powder'), ('Butter', 'Lassi'), ('Butter', 'Coffee Powder'), ('Sugar', 'Tea Powder'), ('Panner', 'Yougurt'), ('Bread', 'Sugar'), ('Bread', 'Milk'), ('Coffee Powder', 'Sugar'), ('Ghee', 'Sugar'), ('Butter', 'Cheese'), ('Cheese', 'Yougurt'), ('Ghee', 'Milk'), ('Panner', 'Sweet'), ('Butter', 'Yougurt'), ('Coffee Powder', 'Milk'), ('Milk', 'Sugar'), ('Panner', 'Tea Powder'), ('Cheese', 'Sweet'), ('Bread', 'Ghee'), ('Butter', 'Sweet'), ('Bread', 'Lassi'), ('Bread', 'Coffee Powder'), ('Cheese', 'Panner'), ('Lassi', 'Yougurt'), ('Butter', 'Panner'), ('Cheese', 'Tea Powder'), ('Coffee Powder', 'Ghee'), ('Butter', 'Tea Powder'), ('Coffee Powder', 'Lassi'), ('Ghee', 'Lassi'), ('Lassi', 'Sweet'), ('Lassi', 'Tea Powder')} "
     ]
    }
   ],
   "source": [
    "print(generate_candidates_fk1_1(unique, unique), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_algorithm(filename, max_length, is_fixed_length, min_support):\n",
    "    data = read_file(filename, True)\n",
    "    one_unique_dict = calculate_support(data, None)\n",
    "    el_one_unique = support_elimination(data, None, min_support, one_unique_dict)\n",
    "    el_k_unique = tuple(sorted([i for i in el_one_unique]))\n",
    "    ## if only n-length itemsets\n",
    "    if is_fixed_length:\n",
    "        for i in range(max_length-1):\n",
    "            if el_k_unique == []:\n",
    "                return []\n",
    "            k_unique = generate_candidates_fk1_1(el_k_unique, el_one_unique)\n",
    "            el_k_unique = support_elimination(data, k_unique, min_support, None)\n",
    "        return el_k_unique\n",
    "    ## if 2 - n-length itemsets\n",
    "    else:\n",
    "        frequent = []\n",
    "        for i in range(max_length-1):\n",
    "            if el_k_unique == []:\n",
    "                return frequent\n",
    "            k_unique = generate_candidates_fk1_1(el_k_unique, el_one_unique)\n",
    "            el_k_unique = support_elimination(data, k_unique, min_support, None)\n",
    "            frequent.extend(el_k_unique)\n",
    "        return frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">Testing Apriori</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori algorithm took appx. 5.58s to run on the GroceryStore.csv dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'GroceryStore.csv'\n",
    "min_support = 0.048\n",
    "max_itemset_length = 4\n",
    "is_fixed_length = True\n",
    "\n",
    "start = timer()\n",
    "apriori_frequent = apriori_algorithm(dataset_name, max_length=max_itemset_length, is_fixed_length=is_fixed_length, min_support=min_support)\n",
    "end = timer()\n",
    "print(f'Apriori algorithm took appx. {\"{:.2f}\".format(end - start)}s to run on the {dataset_name} dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(apriori_frequent, f'Apriori-min_support={min_support}-{datetime.datetime.now()}.csv', reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"color: #7d3434\">NB: in the next sections some functions from the current section will be reused</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rule_props(itemset, data):\n",
    "    T = len(data)\n",
    "    support_count_dict = {}\n",
    "    gen_dict = {}\n",
    "    for item in itemset:\n",
    "        if item not in support_count_dict:\n",
    "            ## calculate support for each set of objects\n",
    "            support_count_dict[item] = calculate_support_count(item, data, True)\n",
    "        ## create all possible subsets of the item / set of objects\n",
    "        all_subsets = chain.from_iterable(combinations(item, i) for i in range(1, len(item)))\n",
    "        for x in all_subsets:\n",
    "            if x not in support_count_dict:\n",
    "                support_count_dict[x] = calculate_support_count(x, data, True)\n",
    "            y = tuple(set(item).difference(set(x)))\n",
    "            if not y in support_count_dict:\n",
    "                support_count_dict[y] = calculate_support_count(y, data, True)\n",
    "            if not (x, y) in gen_dict:\n",
    "                support_count = support_count_dict[item]\n",
    "                support_count_x = support_count_dict[x]\n",
    "                support = support_count / T\n",
    "                support_x = support_count_x / T\n",
    "                ## return props for each X => Y pair\n",
    "                gen_dict[(x, y)] = {\n",
    "                    'support': support,\n",
    "                    'confidence': support_count / support_count_x,\n",
    "                    'lift': support / (support_x * support_count_dict[y] / T)\n",
    "                }\n",
    "    return gen_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rule(min_support, min_confidence, data_file_name, frequent_file_name):\n",
    "    # read the main file and frequent itemsets\n",
    "    data = read_file(data_file_name, skipEmptyColumn=True)\n",
    "    frequent = read_file(frequent_file_name, skipEmptyColumn=False)\n",
    "    characteristics = association_rule_props(frequent, data)\n",
    "    ## eliminate based on the supplied params\n",
    "    return {k: v for k, v in characteristics.items() if v['support'] >= min_support and v['confidence'] >= min_confidence}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">Testing Association Rule</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('Lassi', 'Panner'), ('Sweet',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.5066079295154186,\n",
       "  'lift': 1.1573538072424097},\n",
       " (('Lassi', 'Sweet'), ('Panner',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.49107142857142855,\n",
       "  'lift': 1.129897265666002},\n",
       " (('Panner', 'Sweet'), ('Lassi',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.5049900199600799,\n",
       "  'lift': 1.1644891366016128},\n",
       " (('Lassi',), ('Sweet',)): {'support': 0.20565224333386556,\n",
       "  'confidence': 0.4742268041237113,\n",
       "  'lift': 1.0833786154392868},\n",
       " (('Sweet',), ('Lassi',)): {'support': 0.20565224333386556,\n",
       "  'confidence': 0.4698157942732081,\n",
       "  'lift': 1.0833786154392864},\n",
       " (('Butter',), ('Sugar',)): {'support': 0.20525307360689765,\n",
       "  'confidence': 0.4690749863163656,\n",
       "  'lift': 1.071804684166143},\n",
       " (('Sugar',), ('Butter',)): {'support': 0.20525307360689765,\n",
       "  'confidence': 0.4689894199197373,\n",
       "  'lift': 1.071804684166143},\n",
       " (('Panner',), ('Bread',)): {'support': 0.20357656075363245,\n",
       "  'confidence': 0.46840558412931665,\n",
       "  'lift': 1.069884818892017}}"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_support = 0.1\n",
    "min_confidence = 0.468\n",
    "association_rule(min_support, min_confidence, dataset_name, 'Apriori-min_support=0.1-2021-03-21 00:45:30.864484.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP-Growth Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node:\n",
    "    def __init__(self, entity, parent, count):\n",
    "        self.entity = entity ## item name\n",
    "        self.parent_node = parent\n",
    "        self.child_nodes = {} ## children nodes with the key of item name\n",
    "        self.count = count ## number of passes of the node\n",
    "        self.cond_count = defaultdict(int) ## count of the conditional fp_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_tree(data, min_support):\n",
    "    one_unique_dict = calculate_support(data, None)\n",
    "    el_one_unique = support_elimination(data, None, min_support, one_unique_dict)\n",
    "    el_one_unique_sorted = [i for i in dict(sorted(dct.items(), key=lambda item: item[1], reverse=True))]\n",
    "    node_links = {}\n",
    "    for i in el_one_unique_sorted:\n",
    "        node_links[i] = []\n",
    "    null_node = node(None, None, 1)\n",
    "    for item in data:\n",
    "        item_ordered = sorted(item, key=lambda x: el_one_unique_sorted.index(x))\n",
    "        start_node = null_node\n",
    "        for nd in item_ordered:\n",
    "            if nd in start_node.child_nodes:\n",
    "                start_node.child_nodes[nd].count += 1\n",
    "                start_node = start_node.child_nodes[nd]\n",
    "            else:\n",
    "                this_node = node(nd, start_node, 1)\n",
    "                start_node.child_nodes[nd] = this_node\n",
    "                start_node = this_node\n",
    "                node_links[nd].append(this_node) \n",
    "    node_links = {k: v for k, v in sorted(node_links.items(), key=lambda x: el_one_unique_sorted.index(x[0]))}\n",
    "    return (null_node, node_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_fp_tree(node_links):\n",
    "    for key in node_links:\n",
    "        for nd in node_links[key]:\n",
    "            current = nd\n",
    "            while current.entity != None:\n",
    "                current.parent_node.cond_count[nd.entity] += nd.count\n",
    "                current = current.parent_node              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the rest is unfinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_growth(filename, max_length, min_support):\n",
    "    data = read_file(filename, True)\n",
    "    tree, node_links = fp_tree(data, min_support)\n",
    "    conditional_fp_tree(node_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Milk', 'Ghee', 'Coffee Powder', 'Yougurt', 'Bread', 'Sweet', 'Sugar', 'Butter', 'Cheese', 'Panner', 'Lassi', 'Tea Powder']\n",
      "FP-Growth algorithm took appx. 0.12s to run on the GroceryStore.csv dataset\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "fp_growth('GroceryStore.csv', 5, 0.205)\n",
    "end = timer()\n",
    "print(f'FP-Growth algorithm took appx. {\"{:.2f}\".format(end - start)}s to run on the {dataset_name} dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:gray\">\n",
    "    <span>Judging by the lift, we can see that after purchasing Panner & Sweet customers are more likely to purchase Lassi.</span></br></br>\n",
    "    <span>Frequent gen params:</span>\n",
    "    <ul>\n",
    "        <li>min_support = 0.1</li>\n",
    "        <li>max_itemset_length = 3</li>\n",
    "        <li>is_fixed_length = False</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('Lassi', 'Panner'), ('Sweet',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.5066079295154186,\n",
       "  'lift': 1.1573538072424097},\n",
       " (('Lassi', 'Sweet'), ('Panner',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.49107142857142855,\n",
       "  'lift': 1.129897265666002},\n",
       " (('Panner', 'Sweet'), ('Lassi',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.5049900199600799,\n",
       "  'lift': 1.1644891366016128}}"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "association_rule(min_support=0, min_confidence=0.49, \n",
    "                 data_file_name=dataset_name, frequent_file_name='Apriori-min_support=0.1-2021-03-21 00:45:30.864484.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:gray\">\n",
    "    <span>For 4-itemsets we can see that Panner, Sweet and Tea Powder are to be put together, especially if the task is to increase Lassi sales.</span></br></br>\n",
    "    <span>Frequent gen params:</span>\n",
    "    <ul>\n",
    "        <li>min_support = 0.048</li>\n",
    "        <li>max_itemset_length = 4</li>\n",
    "        <li>is_fixed_length = True</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('Milk', 'Panner', 'Sweet'), ('Lassi',)): {'support': 0.04941721219862685,\n",
       "  'confidence': 0.5254668930390493,\n",
       "  'lift': 1.21170808214417},\n",
       " (('Lassi', 'Panner', 'Tea Powder'),\n",
       "  ('Sweet',)): {'support': 0.04981638192559476, 'confidence': 0.5324232081911263, 'lift': 1.2163292186398043},\n",
       " (('Panner', 'Sweet', 'Tea Powder'),\n",
       "  ('Lassi',)): {'support': 0.04981638192559476, 'confidence': 0.5342465753424658, 'lift': 1.231953719208344}}"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "association_rule(min_support=0, min_confidence=0.52, \n",
    "                 data_file_name=dataset_name, frequent_file_name='Apriori-min_support=0.048-2021-03-20 23:12:47.216416.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:gray\">\n",
    "    <span>Tea Powder, Coffee Powder and Milk (ranked to highest P to lowest) are most likely to go in the basket with Panner when a customer buys Lassi and Sweet.</span></br></br>\n",
    "    <span>Frequent gen params:</span>\n",
    "    <ul>\n",
    "        <li>min_support = 0.0235</li>\n",
    "        <li>max_itemset_length = 5</li>\n",
    "        <li>is_fixed_length = True</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(('Coffee Powder', 'Lassi', 'Panner', 'Tea Powder'), ('Sweet',)): {'confidence': 0.5376344086021505,\n",
      "                                                                    'lift': 1.2282342881908694,\n",
      "                                                                    'support': 0.023950183618074404},\n",
      " (('Coffee Powder', 'Panner', 'Sweet', 'Tea Powder'), ('Lassi',)): {'confidence': 0.5494505494505495,\n",
      "                                                                    'lift': 1.2670135461004386,\n",
      "                                                                    'support': 0.023950183618074404},\n",
      " (('Lassi', 'Milk', 'Panner', 'Tea Powder'), ('Sweet',)): {'confidence': 0.5314183123877917,\n",
      "                                                           'lift': 1.2140335183238153,\n",
      "                                                           'support': 0.02363084783650008},\n",
      " (('Milk', 'Panner', 'Sweet', 'Tea Powder'), ('Lassi',)): {'confidence': 0.5323741007194245,\n",
      "                                                           'lift': 1.227635858912281,\n",
      "                                                           'support': 0.02363084783650008}}\n"
     ]
    }
   ],
   "source": [
    "min_confidence = 0.53\n",
    "rule = association_rule(min_support=0, min_confidence=min_confidence, \n",
    "                 data_file_name=dataset_name, frequent_file_name='Apriori-min_support=0.0235-2021-03-20 23:47:48.888064.csv')\n",
    "pprint.pprint(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can write the acquired rules in a file\n",
    "write_file(rule.keys(), f'Association_rule-confidence={min_confidence}-{datetime.datetime.now()}.csv', reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
