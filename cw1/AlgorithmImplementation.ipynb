{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023638 | Anton Dementyev | Coursework 1: Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import threading\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, skipEmptyColumn):\n",
    "    ds = []\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        filereader = csv.reader(csvfile, delimiter=',')\n",
    "        if skipEmptyColumn:\n",
    "            for row in filereader:\n",
    "                ds.append(tuple(sorted(row[:-1])))\n",
    "        else:\n",
    "            for row in filereader:\n",
    "                ds.append(tuple(sorted(row)))\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Butter', 'Cheese', 'Coffee Powder', 'Ghee', 'Lassi', 'Yougurt'), ('Coffee Powder', 'Ghee'), ('Butter', 'Cheese', 'Lassi', 'Tea Powder'), ('Bread', 'Butter', 'Cheese', 'Coffee Powder', 'Panner', 'Tea Powder'), ('Butter', 'Cheese', 'Coffee Powder', 'Sugar', 'Sweet', 'Yougurt')]\n"
     ]
    }
   ],
   "source": [
    "read = read_file('GroceryStore.csv', True)[:5]\n",
    "print(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(data, filename, reverse):\n",
    "    if reverse:\n",
    "        data.reverse()\n",
    "    if data != [] and data is not None:\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            filewriter = csv.writer(csvfile, delimiter=',')\n",
    "            for row in data:\n",
    "                filewriter.writerow(row)\n",
    "    else:\n",
    "        print('Attempting to write empty dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support_count(instance, data, is_set):\n",
    "    count = 0\n",
    "    if is_set:\n",
    "        for row in data:\n",
    "            if set(instance).issubset(set(row)): \n",
    "                count = count + 1\n",
    "    else:\n",
    "        for row in data:\n",
    "            if instance in row:\n",
    "                count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support(data, items):\n",
    "    dct = {}\n",
    "    lgtn = len(data)\n",
    "    if isinstance(items, set): \n",
    "        for i in items:\n",
    "            support_count = calculate_support_count(i, data, True)\n",
    "            dct[i] = support_count / lgtn\n",
    "    else: \n",
    "        for i in items:\n",
    "            support_count = calculate_support_count(i, data, False)\n",
    "            dct[i] = support_count / lgtn\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_elimination(data, items, minimal_support):\n",
    "    dct = calculate_support(data, items)\n",
    "    support_resistant = []\n",
    "    for key in dct:\n",
    "        if dct[key] >= minimal_support:\n",
    "            support_resistant.append(key)\n",
    "    return support_resistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_level_one(data, sort_by_support_desc):\n",
    "    dct = defaultdict(int)\n",
    "    for y in data:\n",
    "        for x in y:\n",
    "            dct[x] += 1\n",
    "    if sort_by_support_desc:\n",
    "        return tuple([i for i in dict(sorted(dct.items(), key=lambda item: item[1], reverse=True))])\n",
    "    else:\n",
    "        return tuple(sorted([i for i in dct]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bread', 'Butter', 'Cheese', 'Coffee Powder', 'Ghee', 'Lassi', 'Milk', 'Panner', 'Sugar', 'Sweet', 'Tea Powder', 'Yougurt'] "
     ]
    }
   ],
   "source": [
    "data = read_file('GroceryStore.csv', True)\n",
    "unique = find_unique_level_one(data, False)\n",
    "print(support_elimination(data, unique, 0.4), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_fk1_1(previous_step_f_itemset, step_one_f_itemset):\n",
    "    lst = []\n",
    "    if isinstance(previous_step_f_itemset[0], list) or isinstance(previous_step_f_itemset[0], tuple): \n",
    "        for i in previous_step_f_itemset:\n",
    "            for k in step_one_f_itemset:\n",
    "                if k not in i:\n",
    "                    lst.append(tuple(sorted([*i, k])))\n",
    "    else:\n",
    "        for i in previous_step_f_itemset:\n",
    "            for k in step_one_f_itemset:\n",
    "                if k != i:\n",
    "                    lst.append(tuple(sorted((i, k))))\n",
    "    return set(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Lassi', 'Panner'), ('Bread', 'Cheese'), ('Bread', 'Yougurt'), ('Coffee Powder', 'Yougurt'), ('Ghee', 'Yougurt'), ('Panner', 'Sugar'), ('Bread', 'Butter'), ('Bread', 'Sweet'), ('Bread', 'Tea Powder'), ('Bread', 'Panner'), ('Cheese', 'Sugar'), ('Coffee Powder', 'Sweet'), ('Butter', 'Sugar'), ('Ghee', 'Sweet'), ('Ghee', 'Tea Powder'), ('Milk', 'Yougurt'), ('Coffee Powder', 'Panner'), ('Ghee', 'Panner'), ('Tea Powder', 'Yougurt'), ('Coffee Powder', 'Tea Powder'), ('Cheese', 'Milk'), ('Butter', 'Milk'), ('Milk', 'Sweet'), ('Lassi', 'Sugar'), ('Milk', 'Panner'), ('Sugar', 'Yougurt'), ('Sweet', 'Yougurt'), ('Lassi', 'Milk'), ('Milk', 'Tea Powder'), ('Cheese', 'Ghee'), ('Butter', 'Ghee'), ('Cheese', 'Coffee Powder'), ('Sugar', 'Sweet'), ('Cheese', 'Lassi'), ('Sweet', 'Tea Powder'), ('Butter', 'Lassi'), ('Butter', 'Coffee Powder'), ('Sugar', 'Tea Powder'), ('Panner', 'Yougurt'), ('Bread', 'Sugar'), ('Bread', 'Milk'), ('Coffee Powder', 'Sugar'), ('Ghee', 'Sugar'), ('Butter', 'Cheese'), ('Cheese', 'Yougurt'), ('Ghee', 'Milk'), ('Panner', 'Sweet'), ('Butter', 'Yougurt'), ('Coffee Powder', 'Milk'), ('Milk', 'Sugar'), ('Panner', 'Tea Powder'), ('Cheese', 'Sweet'), ('Bread', 'Ghee'), ('Butter', 'Sweet'), ('Bread', 'Lassi'), ('Bread', 'Coffee Powder'), ('Cheese', 'Panner'), ('Lassi', 'Yougurt'), ('Butter', 'Panner'), ('Cheese', 'Tea Powder'), ('Coffee Powder', 'Ghee'), ('Butter', 'Tea Powder'), ('Coffee Powder', 'Lassi'), ('Ghee', 'Lassi'), ('Lassi', 'Sweet'), ('Lassi', 'Tea Powder')} "
     ]
    }
   ],
   "source": [
    "print(generate_candidates_fk1_1(unique, unique), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_algorigth(filename, max_length, min_support):\n",
    "    data = read_file(filename, True)\n",
    "    one_unique = find_unique_level_one(data, False)\n",
    "    if len(one_unique) < max_length:\n",
    "        return\n",
    "    el_one_unique = support_elimination(data, one_unique, min_support)\n",
    "    if el_one_unique == []:\n",
    "        return\n",
    "    lim = 1\n",
    "    frequent = []\n",
    "    el_k_unique = el_one_unique\n",
    "    while(lim < max_length):\n",
    "        if el_k_unique == []:\n",
    "            return frequent\n",
    "        k_unique = generate_candidates_fk1_1(el_k_unique, el_one_unique)\n",
    "        el_k_unique = support_elimination(data, k_unique, min_support)\n",
    "        frequent.extend(el_k_unique)\n",
    "        lim = lim + 1\n",
    "    return frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">Testing Apriori</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori algorithm took appx. 2.23s to run on the GroceryStore.csv dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'GroceryStore.csv'\n",
    "min_support = 0.1\n",
    "max_itemset_length = 5\n",
    "\n",
    "start = timer()\n",
    "apriori_frequent = apriori_algorigth(dataset_name, max_length=max_itemset_length, min_support=min_support)\n",
    "end = timer()\n",
    "print(f'Apriori algorithm took appx. {\"{:.2f}\".format(end - start)}s to run on the {dataset_name} dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(apriori_frequent, f'Apriori-{datetime.datetime.now()}.csv', reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"color: #7d3434\">NB: in the next sections some functions from the current section will be reused</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rule_props(itemset, data):\n",
    "    T = len(data)\n",
    "    support_count_dict = {}\n",
    "    gen_dict = {}\n",
    "    for item in itemset:\n",
    "        if item not in support_count_dict:\n",
    "            support_count_dict[item] = calculate_support_count(item, data, True)\n",
    "        all_subsets = chain.from_iterable(combinations(item, i) for i in range(1, len(item)))\n",
    "        for x in all_subsets:\n",
    "            if x not in support_count_dict:\n",
    "                support_count_dict[x] = calculate_support_count(x, data, True)\n",
    "            y = tuple(set(item).difference(set(x)))\n",
    "            if not y in support_count_dict:\n",
    "                support_count_dict[y] = calculate_support_count(y, data, True)\n",
    "            if not (x, y) in gen_dict:\n",
    "                support_count = support_count_dict[item]\n",
    "                support_count_x = support_count_dict[x]\n",
    "                support = support_count / T\n",
    "                support_x = support_count_x / T\n",
    "                gen_dict[(x, y)] = {\n",
    "                    'support': support,\n",
    "                    'confidence': support_count / support_count_x,\n",
    "                    'lift': support / (support_x * support_count_dict[y] / T)\n",
    "                }\n",
    "    return gen_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rule(min_support, min_confidence, data_file_name, frequent_file_name):\n",
    "    data = read_file(data_file_name, skipEmptyColumn=True)\n",
    "    frequent = read_file(frequent_file_name, skipEmptyColumn=False)\n",
    "    characteristics = association_rule_props(frequent, data)\n",
    "    return {k: v for k, v in characteristics.items() if v['support'] >= min_support and v['confidence'] >= min_confidence}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">Testing Association Rule</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('Lassi', 'Panner'), ('Sweet',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.5066079295154186,\n",
       "  'lift': 1.1573538072424097},\n",
       " (('Lassi', 'Sweet'), ('Panner',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.49107142857142855,\n",
       "  'lift': 1.129897265666002},\n",
       " (('Panner', 'Sweet'), ('Lassi',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.5049900199600799,\n",
       "  'lift': 1.1644891366016128}}"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_support = 0.1\n",
    "min_confidence = 0.48\n",
    "association_rule(min_support, min_confidence, dataset_name, 'Apriori-2021-03-20 17:37:12.342135.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP-Growth Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node:\n",
    "    def __init__(self, entity, parent, count):\n",
    "        self.entity = entity\n",
    "        self.parent_node = parent\n",
    "        self.child_nodes = {}\n",
    "        self.count = count\n",
    "        self.cond_count = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_tree(data, min_support):\n",
    "    one_unique = find_unique_level_one(data, True)\n",
    "    el_one_unique_sorted = support_elimination(data, one_unique, min_support)\n",
    "    node_links = {}\n",
    "    for i in el_one_unique_sorted:\n",
    "        node_links[i] = []\n",
    "    null_node = node(None, None, 1)\n",
    "    for item in data:\n",
    "        item_ordered = sorted(item, key=lambda x: el_one_unique_sorted.index(x))\n",
    "        start_node = null_node\n",
    "        for nd in item_ordered:\n",
    "            if nd in start_node.child_nodes:\n",
    "                start_node.child_nodes[nd].count += 1\n",
    "                start_node = start_node.child_nodes[nd]\n",
    "            else:\n",
    "                this_node = node(nd, start_node, 1)\n",
    "                start_node.child_nodes[nd] = this_node\n",
    "                start_node = this_node\n",
    "                node_links[nd].append(this_node) \n",
    "    node_links = {k: v for k, v in sorted(node_links.items(), key=lambda x: el_one_unique_sorted.index(x[0]))}\n",
    "    return (null_node, node_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_itemsets(node_links):\n",
    "    frequent = [] ## then make it a set\n",
    "    for key in node_links:\n",
    "        for nd in node_links[key]:\n",
    "            current = nd\n",
    "            while current.entity != None:\n",
    "                current.parent_node.cond_count[nd.entity] += nd.count\n",
    "                current = current.parent_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_growth(filename, max_length, min_support):\n",
    "    data = read_file(filename)\n",
    "    tree, node_links = fp_tree(data, min_support)\n",
    "    get_frequent_itemsets(node_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP-Growth algorithm took appx. 0.13s to run on the GroceryStore.csv dataset\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "fp_growth('GroceryStore.csv', 5, 0.205)\n",
    "end = timer()\n",
    "print(f'FP-Growth algorithm took appx. {\"{:.2f}\".format(end - start)}s to run on the {dataset_name} dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
