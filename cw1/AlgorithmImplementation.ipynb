{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023638 | Anton Dementyev | Coursework 1: Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import math\n",
    "import threading\n",
    "import pprint\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, skipEmptyColumn):\n",
    "    ds = []\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        filereader = csv.reader(csvfile, delimiter=',')\n",
    "        if skipEmptyColumn:\n",
    "            for row in filereader:\n",
    "                ## tuple is hashable, which is useful for all further actions\n",
    "                ## sorting by name is default\n",
    "                ds.append(tuple(sorted(row[:-1])))\n",
    "        else:\n",
    "            for row in filereader:\n",
    "                ds.append(tuple(sorted(row)))\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">Testing Read File</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Butter', 'Cheese', 'Coffee Powder', 'Ghee', 'Lassi', 'Yougurt'), ('Coffee Powder', 'Ghee'), ('Butter', 'Cheese', 'Lassi', 'Tea Powder'), ('Bread', 'Butter', 'Cheese', 'Coffee Powder', 'Panner', 'Tea Powder'), ('Butter', 'Cheese', 'Coffee Powder', 'Sugar', 'Sweet', 'Yougurt')]\n"
     ]
    }
   ],
   "source": [
    "read = read_file('GroceryStore.csv', True)[:5]\n",
    "print(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(data, filename, reverse):\n",
    "    # reverse mode can be helpful to create a required order of itemsets\n",
    "    if reverse:\n",
    "        data.reverse()\n",
    "    if data != [] and data is not None:\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            filewriter = csv.writer(csvfile, delimiter=',')\n",
    "            for row in data:\n",
    "                filewriter.writerow(row)\n",
    "    else:\n",
    "        print('Attempting to write empty dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support_count(instance, data, is_set):\n",
    "    count = 0\n",
    "    # if item is a plain string, do not waste memory to make convert it to a set\n",
    "    if is_set:\n",
    "        for row in data:\n",
    "            if set(instance).issubset(set(row)): \n",
    "                count = count + 1\n",
    "    else:\n",
    "        for row in data:\n",
    "            if instance in row:\n",
    "                count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support(data, items):\n",
    "    dct = {}\n",
    "    lgtn = len(data)\n",
    "    ## if not itemset is supplied, decompose the data to the mininum\n",
    "    if items == None:\n",
    "        for i in data:\n",
    "            for j in i:\n",
    "                if j in dct:\n",
    "                    continue\n",
    "                else:\n",
    "                    support_count = calculate_support_count(j, data, False)\n",
    "                    dct[j] = support_count / lgtn\n",
    "    else:\n",
    "        for i in items:\n",
    "            support_count = calculate_support_count(i, data, True)\n",
    "            dct[i] = support_count / lgtn\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_elimination(data, items, minimal_support, support_dict):\n",
    "    ## a support dictitionary can already be supplied, if not - calculate\n",
    "    dct = calculate_support(data, items) if support_dict is None else support_dict\n",
    "    support_resistant = []\n",
    "    for key in dct:\n",
    "        if dct[key] >= minimal_support:\n",
    "            support_resistant.append(key)\n",
    "    return support_resistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_fk1_1(previous_step_f_itemset, step_one_f_itemset):\n",
    "    lst = []\n",
    "    if isinstance(previous_step_f_itemset[0], list) or isinstance(previous_step_f_itemset[0], tuple): \n",
    "        for i in previous_step_f_itemset:\n",
    "            for k in step_one_f_itemset:\n",
    "                if k not in i:\n",
    "                    lst.append(tuple(sorted([*i, k])))\n",
    "    else:\n",
    "        for i in previous_step_f_itemset:\n",
    "            for k in step_one_f_itemset:\n",
    "                if k != i:\n",
    "                    lst.append(tuple(sorted((i, k))))\n",
    "    return set(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">Testing Candidates Generation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Bread', 'Sugar'), ('Cheese', 'Milk'), ('Lassi', 'Sweet'), ('Ghee', 'Lassi'), ('Butter', 'Sweet'), ('Milk', 'Sweet'), ('Lassi', 'Panner'), ('Butter', 'Panner'), ('Milk', 'Panner'), ('Coffee Powder', 'Lassi'), ('Bread', 'Milk'), ('Coffee Powder', 'Ghee'), ('Lassi', 'Tea Powder'), ('Ghee', 'Sugar'), ('Bread', 'Butter'), ('Lassi', 'Yougurt'), ('Milk', 'Yougurt'), ('Butter', 'Tea Powder'), ('Milk', 'Tea Powder'), ('Cheese', 'Sweet'), ('Panner', 'Sugar'), ('Panner', 'Sweet'), ('Butter', 'Yougurt'), ('Coffee Powder', 'Sugar'), ('Cheese', 'Panner'), ('Ghee', 'Milk'), ('Bread', 'Sweet'), ('Cheese', 'Tea Powder'), ('Panner', 'Tea Powder'), ('Cheese', 'Yougurt'), ('Bread', 'Panner'), ('Coffee Powder', 'Milk'), ('Panner', 'Yougurt'), ('Bread', 'Tea Powder'), ('Butter', 'Cheese'), ('Bread', 'Yougurt'), ('Butter', 'Coffee Powder'), ('Ghee', 'Sweet'), ('Butter', 'Lassi'), ('Butter', 'Ghee'), ('Ghee', 'Panner'), ('Lassi', 'Sugar'), ('Coffee Powder', 'Sweet'), ('Ghee', 'Tea Powder'), ('Sugar', 'Sweet'), ('Butter', 'Sugar'), ('Cheese', 'Coffee Powder'), ('Milk', 'Sugar'), ('Cheese', 'Lassi'), ('Ghee', 'Yougurt'), ('Cheese', 'Ghee'), ('Coffee Powder', 'Panner'), ('Bread', 'Cheese'), ('Coffee Powder', 'Tea Powder'), ('Lassi', 'Milk'), ('Sweet', 'Tea Powder'), ('Butter', 'Milk'), ('Bread', 'Lassi'), ('Coffee Powder', 'Yougurt'), ('Bread', 'Ghee'), ('Sugar', 'Tea Powder'), ('Bread', 'Coffee Powder'), ('Cheese', 'Sugar'), ('Sugar', 'Yougurt'), ('Sweet', 'Yougurt'), ('Tea Powder', 'Yougurt')} "
     ]
    }
   ],
   "source": [
    "data = read_file('GroceryStore.csv', True)\n",
    "unique = calculate_support(data, None)\n",
    "el_one_unique = support_elimination(data, None, 0.2, unique)\n",
    "el_one_unique = tuple(sorted([i for i in el_one_unique]))\n",
    "print(generate_candidates_fk1_1(el_one_unique, el_one_unique), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_algorithm(filename, max_length, is_fixed_length, min_support):\n",
    "    data = read_file(filename, True)\n",
    "    ## a trick in order not to calculate support for 1-itemsets two times\n",
    "    one_unique_dict = calculate_support(data, None)\n",
    "    el_one_unique = support_elimination(data, None, min_support, one_unique_dict)\n",
    "    el_k_unique = tuple(sorted([i for i in el_one_unique]))\n",
    "    ## if only n-length itemsets\n",
    "    if is_fixed_length:\n",
    "        for i in range(max_length-1):\n",
    "            if el_k_unique == []:\n",
    "                return []\n",
    "            k_unique = generate_candidates_fk1_1(el_k_unique, el_one_unique)\n",
    "            el_k_unique = support_elimination(data, k_unique, min_support, None)\n",
    "        return el_k_unique\n",
    "    ## if 2 - n-length itemsets\n",
    "    else:\n",
    "        frequent = []\n",
    "        for i in range(max_length-1):\n",
    "            if el_k_unique == []:\n",
    "                return frequent\n",
    "            k_unique = generate_candidates_fk1_1(el_k_unique, el_one_unique)\n",
    "            el_k_unique = support_elimination(data, k_unique, min_support, None)\n",
    "            frequent.extend(el_k_unique)\n",
    "        return frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">Testing Apriori</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori algorithm took appx. 2.16s to run on the GroceryStore.csv dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'GroceryStore.csv'\n",
    "min_support = 0.1\n",
    "max_itemset_length = 3\n",
    "is_fixed_length = False\n",
    "\n",
    "start = timer()\n",
    "apriori_frequent = apriori_algorithm(dataset_name, max_length=max_itemset_length, is_fixed_length=is_fixed_length, min_support=min_support)\n",
    "end = timer()\n",
    "print(f'Apriori algorithm took appx. {\"{:.2f}\".format(end - start)}s to run on the {dataset_name} dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(apriori_frequent, f'Apriori-min_support={min_support}-{datetime.datetime.now()}.csv', reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"color: #7d3434\">NB: in the next sections some functions from the current section will be reused</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rule_props(itemset, data):\n",
    "    T = len(data)\n",
    "    support_count_dict = {}\n",
    "    gen_dict = {}\n",
    "    for item in itemset:\n",
    "        if item not in support_count_dict:\n",
    "            ## calculate support for each set of objects\n",
    "            support_count_dict[item] = calculate_support_count(item, data, True)\n",
    "        ## create all possible subsets of the item / set of objects\n",
    "        all_subsets = chain.from_iterable(combinations(item, i) for i in range(1, len(item)))\n",
    "        for x in all_subsets:\n",
    "            if x not in support_count_dict:\n",
    "                support_count_dict[x] = calculate_support_count(x, data, True)\n",
    "            ## {Y} is {X, Y} - {X}\n",
    "            y = tuple(set(item).difference(set(x)))\n",
    "            if not y in support_count_dict:\n",
    "                support_count_dict[y] = calculate_support_count(y, data, True)\n",
    "            if not (x, y) in gen_dict:\n",
    "                support_count = support_count_dict[item]\n",
    "                support_count_x = support_count_dict[x]\n",
    "                support = support_count / T\n",
    "                support_x = support_count_x / T\n",
    "                ## return props for each X => Y pair\n",
    "                gen_dict[(x, y)] = {\n",
    "                    'support': support,\n",
    "                    'confidence': support_count / support_count_x,\n",
    "                    'lift': support / (support_x * support_count_dict[y] / T)\n",
    "                }\n",
    "    return gen_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rule(min_support, min_confidence, data_file_name, frequent_file_name):\n",
    "    # read the main file and frequent itemsets\n",
    "    data = read_file(data_file_name, skipEmptyColumn=True)\n",
    "    frequent = read_file(frequent_file_name, skipEmptyColumn=False)\n",
    "    characteristics = association_rule_props(frequent, data)\n",
    "    ## eliminate based on the supplied params\n",
    "    return {k: v for k, v in characteristics.items() if v['support'] >= min_support and v['confidence'] >= min_confidence}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">Testing Association Rule</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('Lassi', 'Panner'), ('Sweet',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.5066079295154186,\n",
       "  'lift': 1.1573538072424097},\n",
       " (('Lassi', 'Sweet'), ('Panner',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.49107142857142855,\n",
       "  'lift': 1.129897265666002},\n",
       " (('Panner', 'Sweet'), ('Lassi',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.5049900199600799,\n",
       "  'lift': 1.1644891366016128},\n",
       " (('Lassi',), ('Sweet',)): {'support': 0.20565224333386556,\n",
       "  'confidence': 0.4742268041237113,\n",
       "  'lift': 1.0833786154392868},\n",
       " (('Sweet',), ('Lassi',)): {'support': 0.20565224333386556,\n",
       "  'confidence': 0.4698157942732081,\n",
       "  'lift': 1.0833786154392864},\n",
       " (('Butter',), ('Sugar',)): {'support': 0.20525307360689765,\n",
       "  'confidence': 0.4690749863163656,\n",
       "  'lift': 1.071804684166143},\n",
       " (('Sugar',), ('Butter',)): {'support': 0.20525307360689765,\n",
       "  'confidence': 0.4689894199197373,\n",
       "  'lift': 1.071804684166143},\n",
       " (('Panner',), ('Bread',)): {'support': 0.20357656075363245,\n",
       "  'confidence': 0.46840558412931665,\n",
       "  'lift': 1.069884818892017}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_support = 0.1\n",
    "min_confidence = 0.468\n",
    "association_rule(min_support, min_confidence, dataset_name, 'Apriori-min_support=0.1-2021-03-21 00:45:30.864484.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP-Growth Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tree node\n",
    "class node:\n",
    "    def __init__(self, entity, parent, count):\n",
    "        self.entity = entity ## item name\n",
    "        self.parent_node = parent\n",
    "        self.child_nodes = {} ## children nodes with the key of item name\n",
    "        self.count = count ## number of passes of the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_tree(data, min_support):\n",
    "    one_unique_dict = calculate_support(data, None)\n",
    "    el_one_unique = support_elimination(data, None, min_support, one_unique_dict)\n",
    "    ## sort by support, descending order\n",
    "    el_one_unique_sorted = [i for i in dict(sorted(one_unique_dict.items(), key=lambda item: item[1], reverse=True))]\n",
    "    ## sorted linked lists of tail nodes\n",
    "    node_links = {}\n",
    "    for i in el_one_unique_sorted:\n",
    "        ## the list itself\n",
    "        node_links[i] = []\n",
    "    null_node = node(None, None, 1)\n",
    "    for item in data:\n",
    "        ## sorting items based on the order of unique 1-itemsets\n",
    "        item_ordered = sorted(item, key=lambda x: el_one_unique_sorted.index(x))\n",
    "        ## root of the tree\n",
    "        start_node = null_node\n",
    "        for nd in item_ordered:\n",
    "            ## if the path has been created, just update the count\n",
    "            if nd in start_node.child_nodes:\n",
    "                start_node.child_nodes[nd].count += 1\n",
    "                start_node = start_node.child_nodes[nd]\n",
    "            else:\n",
    "                ## create a new node if there was not path\n",
    "                this_node = node(nd, start_node, 1)\n",
    "                start_node.child_nodes[nd] = this_node\n",
    "                start_node = this_node\n",
    "                ## update the \"linked list\"\n",
    "                node_links[nd].append(this_node) \n",
    "    ## sort dict based on frequency\n",
    "#     node_links = {k: v for k, v in sorted(node_links.items(), key=lambda x: el_one_unique_sorted.index(x[0]))}\n",
    "    return (null_node, node_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_frequent_itemsets(node_links, min_support_count):\n",
    "    lst = []\n",
    "    for key in node_links:\n",
    "        ## this is the list of tail nodes\n",
    "        patterns = defaultdict(int)\n",
    "        freq = []\n",
    "        ## for each node in the linked list\n",
    "        for nd in node_links[key]:\n",
    "            current = nd\n",
    "            prefix = []\n",
    "            ## go up until reaches null node\n",
    "            while current.parent_node != None:\n",
    "                prefix.append(current.entity)\n",
    "                current = current.parent_node\n",
    "            ## create all possible links in a branch, 1-itemsets are EXCLUDED\n",
    "            all_subsets = chain.from_iterable(combinations(prefix, i) for i in range(2, len(prefix)+1))\n",
    "            ## put in the buffer dict\n",
    "            for i in all_subsets:\n",
    "                patterns[i] += nd.count\n",
    "                if patterns[i] >= min_support_count:\n",
    "                    ## if it reaches the min support count, put it in the frequent list and make sure it doesn't get put there again\n",
    "                    ## the latter is ensure by making the count -infinity\n",
    "                    patterns[i] = -math.inf \n",
    "                    freq.append(i)\n",
    "            lst.extend(freq)\n",
    "    return set(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_growth(filename, max_length, is_fixed_length, min_support):\n",
    "    data = read_file(filename, True)\n",
    "    tree, node_links = fp_tree(data, min_support)\n",
    "    min_support_count = min_support * len(data)\n",
    "    ## param-based return, implemented to match apriori function\n",
    "    if max_length is None:\n",
    "        return fp_frequent_itemsets(node_links, min_support_count)\n",
    "    else:\n",
    "        if is_fixed_length:\n",
    "            return [x for x in fp_frequent_itemsets(node_links, min_support_count) if len(x) == max_length]\n",
    "        else:\n",
    "            return [x for x in fp_frequent_itemsets(node_links, min_support_count) if len(x) <= max_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">Testing FP</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP-Growth algorithm took appx. 0.41s to run on the GroceryStore.csv dataset\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "fp = fp_growth('GroceryStore.csv', max_length=5, is_fixed_length=True, min_support=0.0235)\n",
    "end = timer()\n",
    "print(f'FP-Growth algorithm took appx. {\"{:.2f}\".format(end - start)}s to run on the {dataset_name} dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">Testing FP time: ~ the most demanding case for this dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP-Growth algorithm took appx. 0.66s to run on the GroceryStore.csv dataset\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "fp = fp_growth('GroceryStore.csv', max_length=None, is_fixed_length=False, min_support=0.005)\n",
    "end = timer()\n",
    "print(f'FP-Growth algorithm took appx. {\"{:.2f}\".format(end - start)}s to run on the {dataset_name} dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">Writing into a file</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = sorted(fp, key=len, reverse=True)\n",
    "write_file(fp, f'FP-Growth-min_support={min_support}-{datetime.datetime.now()}.csv', reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments on the Dataset\n",
    "##### Conducted with both Apriori and FP-Growth. All the parameters can be inserted in the Algorithm functions and run again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:gray\">\n",
    "    <span>Judging by the lift, we can see that after purchasing Panner & Sweet customers are more likely to purchase Lassi.</span></br></br>\n",
    "    <span>Params:</span>\n",
    "    <ul>\n",
    "        <li>min_support = 0.1</li>\n",
    "        <li>max_itemset_length = 3</li>\n",
    "        <li>is_fixed_length = False</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('Lassi', 'Panner'), ('Sweet',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.5066079295154186,\n",
       "  'lift': 1.1573538072424097},\n",
       " (('Lassi', 'Sweet'), ('Panner',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.49107142857142855,\n",
       "  'lift': 1.129897265666002},\n",
       " (('Panner', 'Sweet'), ('Lassi',)): {'support': 0.10098994092288041,\n",
       "  'confidence': 0.5049900199600799,\n",
       "  'lift': 1.1644891366016128}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "association_rule(min_support=0, min_confidence=0.49, \n",
    "                 data_file_name=dataset_name, frequent_file_name='Apriori-min_support=0.1-2021-03-21 00:45:30.864484.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:gray\">\n",
    "    <span>For 4-itemsets we can see that Panner, Sweet and Tea Powder are to be put together, especially if the task is to increase Lassi sales.</span></br></br>\n",
    "    <span>Params:</span>\n",
    "    <ul>\n",
    "        <li>min_support = 0.048</li>\n",
    "        <li>max_itemset_length = 4</li>\n",
    "        <li>is_fixed_length = True</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('Milk', 'Panner', 'Sweet'), ('Lassi',)): {'support': 0.04941721219862685,\n",
       "  'confidence': 0.5254668930390493,\n",
       "  'lift': 1.21170808214417},\n",
       " (('Lassi', 'Panner', 'Tea Powder'),\n",
       "  ('Sweet',)): {'support': 0.04981638192559476, 'confidence': 0.5324232081911263, 'lift': 1.2163292186398043},\n",
       " (('Panner', 'Sweet', 'Tea Powder'),\n",
       "  ('Lassi',)): {'support': 0.04981638192559476, 'confidence': 0.5342465753424658, 'lift': 1.231953719208344}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "association_rule(min_support=0, min_confidence=0.52, \n",
    "                 data_file_name=dataset_name, frequent_file_name='Apriori-min_support=0.048-2021-03-20 23:12:47.216416.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:gray\">\n",
    "    <span>Tea Powder, Coffee Powder and Milk (ranked to highest P to lowest) are most likely to go in the basket with Panner when a customer buys Lassi and Sweet.</span></br></br>\n",
    "    <span>Params:</span>\n",
    "    <ul>\n",
    "        <li>min_support = 0.0235</li>\n",
    "        <li>max_itemset_length = 5</li>\n",
    "        <li>is_fixed_length = True</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(('Coffee Powder', 'Lassi', 'Panner', 'Tea Powder'), ('Sweet',)): {'confidence': 0.5376344086021505,\n",
      "                                                                    'lift': 1.2282342881908694,\n",
      "                                                                    'support': 0.023950183618074404},\n",
      " (('Coffee Powder', 'Panner', 'Sweet', 'Tea Powder'), ('Lassi',)): {'confidence': 0.5494505494505495,\n",
      "                                                                    'lift': 1.2670135461004386,\n",
      "                                                                    'support': 0.023950183618074404},\n",
      " (('Lassi', 'Milk', 'Panner', 'Tea Powder'), ('Sweet',)): {'confidence': 0.5314183123877917,\n",
      "                                                           'lift': 1.2140335183238153,\n",
      "                                                           'support': 0.02363084783650008},\n",
      " (('Milk', 'Panner', 'Sweet', 'Tea Powder'), ('Lassi',)): {'confidence': 0.5323741007194245,\n",
      "                                                           'lift': 1.227635858912281,\n",
      "                                                           'support': 0.02363084783650008}}\n"
     ]
    }
   ],
   "source": [
    "min_confidence = 0.53\n",
    "rule = association_rule(min_support=0, min_confidence=min_confidence, \n",
    "                 data_file_name=dataset_name, frequent_file_name='FP-Growth-min_support=0.02-2021-03-22 16:52:48.530205.csv')\n",
    "pprint.pprint(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can write the acquired rules in a file\n",
    "write_file(rule.keys(), f'Association_rule-confidence={min_confidence}-{datetime.datetime.now()}.csv', reverse=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
